# Kioku Architecture Evaluation & Comparison

> Drafted: 2026-02-25 | Based on real test data and industry research

---

## 1. Overall Architecture Assessment

### Kioku vs GraphRAG Solutions Comparison

| Dimension | Microsoft GraphRAG | LightRAG | nano-graphrag | **Kioku** |
|---|---|---|---|---|
| **Approach** | Hierarchical communities (Leiden) | Entity-relationship pairs + embeddings | Minimal (~1100 LOC) graph RAG | Tri-hybrid RRF fusion |
| **Graph Construction** | LLM extraction | LLM extraction | LLM + DSPy | LLM extraction (Claude Haiku) |
| **Search Strategy** | Local + Global + DRIFT | Naive/Local/Global/Hybrid | Naive/Local/Global | BM25 + Vector + Graph ‚Üí RRF |
| **Incremental Updates** | ‚ùå Full rebuild required | ‚úÖ Supported | Partial | ‚úÖ Per-entry insert |
| **Graph Store** | NetworkX (in-memory) | Custom / various | Faiss + Neo4j | FalkorDB |
| **Vector Store** | Internal | Integrated | Faiss / custom | ChromaDB |
| **Multi-hop Reasoning** | ‚úÖ Strong (community summaries) | Medium | Medium | ‚úÖ 2-hop + shortest paths |
| **Indexing Cost** | üí∞üí∞üí∞ Very high | üí∞ Low | üí∞ Low | üí∞ Low (per-entry) |
| **Query Cost** | üí∞üí∞ High (LLM for global) | üí∞ Low | üí∞ Low | üí∞üí∞ Medium (auto-extract LLM) |
| **Lines of Code** | ~10,000+ | ~5,000 | ~1,100 | ~2,500 |
| **Target Use Case** | Enterprise analysis | General RAG | Hackable prototype | Personal memory agent |

### Kioku's Unique Strengths

1. **RRF Tri-Hybrid Fusion** ‚Äî Unlike others that route to different search modes (Local vs Global), Kioku FUSES all three legs (BM25 + Vector + Graph) every query. This is more robust for personal memory where queries are unpredictable.

2. **Per-entry Incremental Insert** ‚Äî Save a memory ‚Üí instantly searchable. No batch rebuild needed. Most Graph RAG solutions (Microsoft) require full re-indexing.

3. **Auto-Extract on Both Sides** ‚Äî Kioku extracts entities on save AND on search, using the same canonical vocabulary. This creates a consistent entity namespace.

4. **Content Hash Linking** ‚Äî Universal dedup key across all stores. Graph edge evidence ‚Üí hydrate from SQLite. This is simpler and more reliable than Microsoft's community-based approach.

5. **Budget System** ‚Äî Explicit 20-entry heavyweight cap prevents context overflow. Most solutions don't have response budget management.

### Kioku's Weaknesses

1. **No Community Detection** ‚Äî Microsoft GraphRAG's Leiden communities enable global summarization ("What are the main themes?"). Kioku can't answer corpus-level questions.

2. **Single-hop Dominant** ‚Äî While Kioku does 2-hop traversal, it doesn't do deep multi-hop reasoning. For "How is A indirectly connected to C through B, D, E?", Microsoft GraphRAG excels.

3. **No Query Routing** ‚Äî LightRAG has Naive/Local/Global/Hybrid modes. Kioku always runs all 3 legs, which is thorough but not efficient for simple queries.

4. **Auto-Extract Adds Latency** ‚Äî ~3s per search for LLM extraction. LightRAG/nano-graphrag don't have this cost on the search side.

---

## 2. Component Evaluation

### 2.1 Entity Extraction: Claude Haiku 3 (claude-3-haiku-20240307)

**Current model:** `claude-3-haiku-20240307` (original Haiku)

| Benchmark | Claude Haiku 3 | Claude Haiku 4.5 | GPT-4o Mini |
|---|---|---|---|
| NER F1 (ProLLM) | ~0.79 | ~0.85 (est.) | 0.836 |
| Complex extraction | Good | Strong | Good |
| Latency | ~1.5s | ~1.2s | ~0.8s |
| Input cost / 1M tokens | $0.25 | $1.00 | $0.15 |
| Output cost / 1M tokens | $1.25 | $5.00 | $0.60 |

**‚ö†Ô∏è ISSUE: Kioku uses the OLD Haiku 3, not Haiku 4.5!**

**Recommendation:** 

| Option | Model | Benefit | Trade-off |
|---|---|---|---|
| üèÜ Best quality | `claude-haiku-4-5-20251001` | Better extraction, same format | ~4x cost increase |
| üí∞ Best cost | `gpt-4o-mini` | Cheapest, fast | Slightly less nuanced |
| ‚öñÔ∏è Balanced | `claude-haiku-4-5-20251001` | Best for Vietnamese text | Worth the cost for personal data |

**Verdict:** Upgrade to **Claude Haiku 4.5** is recommended. Vietnamese text extraction benefits from Claude's stronger multilingual handling. The cost increase (~4x) is negligible for personal use (~10-50 entries/day = <$0.01/day).

### 2.2 Embedding: Ollama nomic-embed-text (768d)

**Current model:** `nomic-embed-text` via Ollama (local)

| Model | Dims | MTEB Score | Multilingual | Local | Cost |
|---|---|---|---|---|---|
| **nomic-embed-text** | 768 | 62.4 | Weak | ‚úÖ Ollama | Free |
| bge-m3 | 1024 | 72.0 | ‚úÖ Strong | ‚úÖ Ollama | Free |
| mxbai-embed-large | 1024 | 64.7 | Medium | ‚úÖ Ollama | Free |
| text-embedding-3-large | 3072 | 64.6 | ‚úÖ Strong | ‚ùå API | $0.13/1M |
| voyage-3-large | 1024 | ~68 | ‚úÖ Strong | ‚ùå API | $0.06/1M |
| Qwen3-Embedding-8B | 4096 | 70.6 | ‚úÖ Best | ‚ùå API/vLLM | Free/GPU |

**‚ö†Ô∏è ISSUE: nomic-embed-text is weak for Vietnamese!**

For a Vietnamese personal diary system, multilingual embedding is critical. nomic-embed-text was primarily trained on English text.

**Recommendation:**

| Option | Model | Benefit | How to use |
|---|---|---|---|
| üèÜ Best for Vietnamese | `bge-m3` | 72% MTEB, tri-modal (dense+sparse+multi-vec), strong multilingual | `ollama pull bge-m3` ‚Äî drop-in replacement |
| ‚öñÔ∏è Good balance | `mxbai-embed-large` | Higher accuracy than nomic, Ollama-native | `ollama pull mxbai-embed-large` |
| üíé Premium | `voyage-3-large` | Best commercial, multilingual | API only |

**Verdict:** Switch to **bge-m3** via Ollama. It's a drop-in replacement (just change model name), free, local, and significantly better for Vietnamese + multilingual content. Expected vector search contribution to jump from 16% to ~30%+.

### 2.3 Vector Store: ChromaDB

| Database | Scalability | Latency | Hybrid Search | Maturity | Best For |
|---|---|---|---|---|---|
| **ChromaDB** | <1M vectors | Medium | ‚ùå No | Growing | Prototyping |
| Qdrant | 1M-100M | Very low | ‚ùå No | Production | Performance |
| Weaviate | 1M-100M | Medium | ‚úÖ BM25+Vector | Mature | Hybrid search |
| Milvus | >100M | Low | ‚úÖ Yes | Enterprise | Massive scale |

**Assessment:** ChromaDB is appropriate for Kioku's scale (~1000-10000 entries). Personal diary doesn't need enterprise vector DB. ChromaDB's simplicity and Python-first API is the right choice.

**Verdict:** ‚úÖ **ChromaDB is fine for personal use.** If scaling to multi-user SaaS, consider Qdrant.

### 2.4 Graph Store: FalkorDB

| Database | Latency | GraphRAG Focus | Multi-tenancy | Community |
|---|---|---|---|---|
| **FalkorDB** | Ultra-low | ‚úÖ Dedicated SDK | ‚úÖ Multi-graph | Growing |
| Neo4j | Medium | ‚úÖ Strong ecosystem | Via labels | Mature |
| Memgraph | Low | Growing | Via labels | Growing |

**Assessment:** FalkorDB is an excellent choice for Kioku:
- Ultra-low latency for traversals (GraphBLAS engine)
- Native multi-graph for user isolation (`kioku_{user_id}`)
- Designed specifically for AI/RAG workloads
- Redis-compatible protocol, simple deployment

**Verdict:** ‚úÖ **FalkorDB is the optimal choice.** Better than Neo4j for this use case (lower latency, less overhead).

### 2.5 Keyword Index: SQLite FTS5

**Assessment:** Perfect for the use case. Zero deployment, embedded, fast BM25, and serves as primary document store for hydration. No alternatives needed.

**Verdict:** ‚úÖ **Optimal.**

---

## 3. Model Optimization Roadmap

### Priority 1: Upgrade Embedding Model (HIGH IMPACT, LOW EFFORT)

```diff
- Current: nomic-embed-text (768d, English-focused)
+ Target:  bge-m3 (1024d, multilingual, 72% MTEB)
```

**Expected impact:** Vector search contribution 16% ‚Üí ~30%+ (especially for Vietnamese queries)  
**Effort:** Change 1 config value + re-embed existing memories  
**Cost:** Free (Ollama)

### Priority 2: Upgrade Extraction Model (MEDIUM IMPACT, LOW EFFORT)

```diff
- Current: claude-3-haiku-20240307 (old Haiku 3)
+ Target:  claude-haiku-4-5-20251001 (Haiku 4.5)
```

**Expected impact:** Better entity extraction, relationship quality, event_time parsing  
**Effort:** Change 1 config value  
**Cost:** ~4x increase but still negligible ($0.01/day for personal use)

### Priority 3: Add Query Routing (MEDIUM IMPACT, MEDIUM EFFORT)

For simple queries ("M·∫π t√¥i t√™n g√¨?"), running all 3 search legs + auto-extract is overkill. Consider:
- Classify query complexity before search
- Simple ‚Üí BM25 only (fast)
- Complex ‚Üí full tri-hybrid (thorough)

**Expected impact:** Reduce latency from ~25s to ~8s for simple queries  
**Effort:** Add query classifier (could be rule-based)

### Priority 4: Cache Canonical Entities (LOW IMPACT, LOW EFFORT)

Auto-extract calls `get_canonical_entities(limit=50)` every search. Cache this for 5 minutes.

**Expected impact:** Save ~200ms per search  
**Effort:** Add simple TTL cache

---

## 4. Summary Scorecard

| Component | Current Choice | Rating | Action |
|---|---|---|---|
| **Architecture** (Tri-hybrid RRF) | Custom | ‚≠ê‚≠ê‚≠ê‚≠ê | Good for personal memory |
| **Graph DB** (FalkorDB) | FalkorDB | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Optimal |
| **Vector DB** (ChromaDB) | ChromaDB | ‚≠ê‚≠ê‚≠ê‚≠ê | Fine for scale |
| **Keyword Index** (SQLite FTS5) | SQLite | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Optimal |
| **Embedding** (nomic-embed-text) | nomic-embed-text | ‚≠ê‚≠ê | **UPGRADE to bge-m3** üî¥ |
| **Extraction** (Claude Haiku 3) | claude-3-haiku | ‚≠ê‚≠ê‚≠ê | **UPGRADE to Haiku 4.5** üü° |
| **RRF Reranking** | Custom | ‚≠ê‚≠ê‚≠ê‚≠ê | Good |
| **Auto-Extract** | Custom | ‚≠ê‚≠ê‚≠ê‚≠ê | Novel approach |
| **Query Enrichment** | Agent-level | ‚≠ê‚≠ê‚≠ê‚≠ê | Good |
| **Budget System** | Custom | ‚≠ê‚≠ê‚≠ê‚≠ê | Good |

**Overall:** Kioku's architecture is solid and well-suited for personal memory. The two critical upgrades are the **embedding model** (bge-m3) and **extraction model** (Haiku 4.5), both of which are low-effort, high-impact changes.
